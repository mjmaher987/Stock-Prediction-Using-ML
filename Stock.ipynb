{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnpHYikW696b1mIzkd6h8l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjmaher987/Stock-Prediction-Using-ML/blob/main/Stock.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NBEATS"
      ],
      "metadata": {
        "id": "MZZagfKTJ-1V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "whfEf41xG0hA",
        "outputId": "b53d9aec-9796-4d19-b546-82a6a6150c14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from tensorflow.keras.layers import Input, Dense, MultiHeadAttention, Reshape, Flatten, LSTM\n",
        "\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "# Load your DataFrame\n",
        "data = pd.read_csv('/content/drive/MyDrive/EURUSD_5min.csv')\n",
        "\n",
        "# Preprocessing\n",
        "scaler = MinMaxScaler()\n",
        "data['close'] = scaler.fit_transform(data['close'].values.reshape(-1, 1))\n",
        "\n",
        "# Splitting into training and testing sets\n",
        "train_size = int(0.8 * len(data))\n",
        "train_data, test_data = data[:train_size], data[train_size:]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for N-BEATS\n",
        "window_size = 10\n",
        "X, y = [], []\n",
        "for i in range(len(data) - window_size):\n",
        "    X.append(data['close'][i:i+window_size])\n",
        "    y.append(data['close'][i+window_size])\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "train_size = int(0.8 * len(X))\n",
        "train_X, test_X = X[:train_size], X[train_size:]\n",
        "train_y, test_y = y[:train_size], y[train_size:]\n",
        "\n",
        "# Build N-BEATS model\n",
        "def build_nbeats(input_shape, output_shape, num_blocks=4, num_neurons=128):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    forecasts = []\n",
        "    for _ in range(num_blocks):\n",
        "        for _ in range(4):  # Directly following the N-BEATS architecture\n",
        "            x = Dense(num_neurons, activation='relu')(x)\n",
        "        forecast = Dense(output_shape[0])(x)\n",
        "        forecasts.append(forecast)\n",
        "    outputs = forecasts[0]  # Use the forecast from the first block as output\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "input_shape = (window_size, 1)\n",
        "output_shape = (1,)\n",
        "nbeats_model = build_nbeats(input_shape, output_shape)\n",
        "nbeats_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train N-BEATS model\n",
        "nbeats_model.fit(train_X, train_y, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# Make predictions using the trained model\n",
        "test_predictions = nbeats_model.predict(test_X)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14tLBzAMfiTa",
        "outputId": "e7df7250-3a07-452a-fb2c-a2065dbba827"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "125/125 [==============================] - 3s 8ms/step - loss: 0.0774\n",
            "Epoch 2/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0591\n",
            "Epoch 3/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.0594\n",
            "Epoch 4/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0594\n",
            "Epoch 5/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0587\n",
            "Epoch 6/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0586\n",
            "Epoch 7/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0584\n",
            "Epoch 8/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0585\n",
            "Epoch 9/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0587\n",
            "Epoch 10/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0586\n",
            "Epoch 11/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.0591\n",
            "Epoch 12/50\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.0583\n",
            "Epoch 13/50\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.0584\n",
            "Epoch 14/50\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.0582\n",
            "Epoch 15/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.0584\n",
            "Epoch 16/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0582\n",
            "Epoch 17/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0583\n",
            "Epoch 18/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0582\n",
            "Epoch 19/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0584\n",
            "Epoch 20/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0583\n",
            "Epoch 21/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0583\n",
            "Epoch 22/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0582\n",
            "Epoch 23/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0583\n",
            "Epoch 24/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0583\n",
            "Epoch 25/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0586\n",
            "Epoch 26/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0582\n",
            "Epoch 27/50\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.0584\n",
            "Epoch 28/50\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0586\n",
            "Epoch 29/50\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.0581\n",
            "Epoch 30/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.0581\n",
            "Epoch 31/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0583\n",
            "Epoch 32/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0581\n",
            "Epoch 33/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0582\n",
            "Epoch 34/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0582\n",
            "Epoch 35/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0582\n",
            "Epoch 36/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0585\n",
            "Epoch 37/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0582\n",
            "Epoch 38/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0584\n",
            "Epoch 39/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0583\n",
            "Epoch 40/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0584\n",
            "Epoch 41/50\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.0583\n",
            "Epoch 42/50\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.0582\n",
            "Epoch 43/50\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.0582\n",
            "Epoch 44/50\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.0582\n",
            "Epoch 45/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.0583\n",
            "Epoch 46/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0580\n",
            "Epoch 47/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0580\n",
            "Epoch 48/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0582\n",
            "Epoch 49/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0581\n",
            "Epoch 50/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0583\n",
            "32/32 [==============================] - 0s 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reshaped_test_predictions = test_predictions[:, -1, :]"
      ],
      "metadata": {
        "id": "ILOv3ECcfC-x"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae = mean_absolute_error(test_y, reshaped_test_predictions)\n",
        "rmse = mean_squared_error(test_y, reshaped_test_predictions, squared=False)\n",
        "print(f\"Mean Absolute Error: {mae}\")\n",
        "print(f\"Root Mean Squared Error: {rmse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-OY7UI1d_mm",
        "outputId": "2a3bf184-b842-4498-c340-18c4a77a75ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error: 0.24873775814261148\n",
            "Root Mean Squared Error: 0.26343894913971233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NHEATS"
      ],
      "metadata": {
        "id": "zhHMx5QtKDDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X = data[['open', 'high', 'low', 'volume']].values\n",
        "y = data['close'].values\n",
        "\n",
        "train_size = int(0.7 * len(X))\n",
        "val_size = int(0.15 * len(X))\n",
        "train_X, test_X, train_y, test_y = X[:train_size], X[train_size:], y[:train_size], y[train_size:]\n",
        "train_X, val_X, train_y, val_y = train_X[:train_size - val_size], train_X[train_size - val_size:], \\\n",
        "                                  train_y[:train_size - val_size], train_y[train_size - val_size:]\n",
        "\n",
        "# Normalize data\n",
        "scaler_X = MinMaxScaler()\n",
        "train_X = scaler_X.fit_transform(train_X)\n",
        "val_X = scaler_X.transform(val_X)\n",
        "test_X = scaler_X.transform(test_X)\n",
        "\n",
        "scaler_y = MinMaxScaler()\n",
        "train_y = scaler_y.fit_transform(train_y.reshape(-1, 1)).flatten()\n",
        "val_y = scaler_y.transform(val_y.reshape(-1, 1)).flatten()\n",
        "test_y = scaler_y.transform(test_y.reshape(-1, 1)).flatten()\n",
        "\n",
        "# NHITS Model\n",
        "def build_nhits(input_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "nhits_model = build_nhits(train_X.shape[1])\n",
        "nhits_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "nhits_model.fit(train_X, train_y, epochs=50, batch_size=32, validation_data=(val_X, val_y), verbose=1)\n",
        "\n",
        "nhits_predictions = nhits_model.predict(test_X)\n",
        "\n",
        "# Calculate MAE and RMSE for NHITS\n",
        "nhits_mae = mean_absolute_error(test_y, nhits_predictions)\n",
        "nhits_rmse = mean_squared_error(test_y, nhits_predictions, squared=False)\n",
        "print(f\"NHITS Mean Absolute Error: {nhits_mae}\")\n",
        "print(f\"NHITS Root Mean Squared Error: {nhits_rmse}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARgE_1C1KEX5",
        "outputId": "94ce8497-a462-4492-bd8e-949fc2ef8176"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0115 - val_loss: 1.4175e-04\n",
            "Epoch 2/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 4.4874e-05 - val_loss: 3.0685e-05\n",
            "Epoch 3/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.1515e-05 - val_loss: 2.7826e-05\n",
            "Epoch 4/50\n",
            "86/86 [==============================] - 0s 4ms/step - loss: 1.9430e-05 - val_loss: 2.8964e-05\n",
            "Epoch 5/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.8558e-05 - val_loss: 2.0280e-05\n",
            "Epoch 6/50\n",
            "86/86 [==============================] - 0s 4ms/step - loss: 1.8451e-05 - val_loss: 1.9499e-05\n",
            "Epoch 7/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.7363e-05 - val_loss: 1.8488e-05\n",
            "Epoch 8/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.7405e-05 - val_loss: 2.5102e-05\n",
            "Epoch 9/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 1.6817e-05 - val_loss: 1.7682e-05\n",
            "Epoch 10/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 1.6171e-05 - val_loss: 1.6421e-05\n",
            "Epoch 11/50\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 1.6741e-05 - val_loss: 2.0711e-05\n",
            "Epoch 12/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 1.6975e-05 - val_loss: 2.2299e-05\n",
            "Epoch 13/50\n",
            "86/86 [==============================] - 1s 15ms/step - loss: 1.5777e-05 - val_loss: 1.6918e-05\n",
            "Epoch 14/50\n",
            "86/86 [==============================] - 0s 4ms/step - loss: 1.5395e-05 - val_loss: 1.4875e-05\n",
            "Epoch 15/50\n",
            "86/86 [==============================] - 0s 4ms/step - loss: 1.6614e-05 - val_loss: 1.4925e-05\n",
            "Epoch 16/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 1.4466e-05 - val_loss: 1.4718e-05\n",
            "Epoch 17/50\n",
            "86/86 [==============================] - 0s 4ms/step - loss: 1.5843e-05 - val_loss: 1.6529e-05\n",
            "Epoch 18/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.4314e-05 - val_loss: 1.4375e-05\n",
            "Epoch 19/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.4079e-05 - val_loss: 1.4824e-05\n",
            "Epoch 20/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.7299e-05 - val_loss: 1.3983e-05\n",
            "Epoch 21/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.4616e-05 - val_loss: 1.3598e-05\n",
            "Epoch 22/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.5752e-05 - val_loss: 1.6934e-05\n",
            "Epoch 23/50\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 1.5112e-05 - val_loss: 2.5374e-05\n",
            "Epoch 24/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.6197e-05 - val_loss: 1.3022e-05\n",
            "Epoch 25/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.3488e-05 - val_loss: 1.2187e-05\n",
            "Epoch 26/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.4830e-05 - val_loss: 1.4104e-05\n",
            "Epoch 27/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.3633e-05 - val_loss: 1.2731e-05\n",
            "Epoch 28/50\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 1.4187e-05 - val_loss: 1.2411e-05\n",
            "Epoch 29/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.6936e-05 - val_loss: 2.4395e-05\n",
            "Epoch 30/50\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 1.4728e-05 - val_loss: 1.3615e-05\n",
            "Epoch 31/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.3973e-05 - val_loss: 1.5866e-05\n",
            "Epoch 32/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.5745e-05 - val_loss: 1.4212e-05\n",
            "Epoch 33/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.3106e-05 - val_loss: 1.3131e-05\n",
            "Epoch 34/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.2814e-05 - val_loss: 1.3473e-05\n",
            "Epoch 35/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.5303e-05 - val_loss: 1.2019e-05\n",
            "Epoch 36/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.4115e-05 - val_loss: 1.4249e-05\n",
            "Epoch 37/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.5438e-05 - val_loss: 1.0757e-05\n",
            "Epoch 38/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.2363e-05 - val_loss: 1.0803e-05\n",
            "Epoch 39/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.6316e-05 - val_loss: 1.0871e-05\n",
            "Epoch 40/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.3159e-05 - val_loss: 1.0478e-05\n",
            "Epoch 41/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.4320e-05 - val_loss: 1.3150e-05\n",
            "Epoch 42/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.4004e-05 - val_loss: 1.0428e-05\n",
            "Epoch 43/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.2301e-05 - val_loss: 9.9796e-06\n",
            "Epoch 44/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.1526e-05 - val_loss: 1.7858e-05\n",
            "Epoch 45/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.1608e-05 - val_loss: 1.2144e-05\n",
            "Epoch 46/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.6339e-05 - val_loss: 3.3364e-05\n",
            "Epoch 47/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.3722e-05 - val_loss: 1.0086e-05\n",
            "Epoch 48/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.5524e-05 - val_loss: 1.6517e-05\n",
            "Epoch 49/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 2.2215e-05 - val_loss: 2.4148e-05\n",
            "Epoch 50/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.8563e-05 - val_loss: 1.9920e-05\n",
            "47/47 [==============================] - 0s 2ms/step\n",
            "NHITS Mean Absolute Error: 0.003578424470980676\n",
            "NHITS Root Mean Squared Error: 0.005121703555336075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RNN"
      ],
      "metadata": {
        "id": "QTwNc2NkLc1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN Model\n",
        "def build_rnn(input_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.LSTM(64, input_shape=(input_shape, 1), activation='relu'),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "train_X_rnn = train_X.reshape(train_X.shape[0], train_X.shape[1], 1)\n",
        "val_X_rnn = val_X.reshape(val_X.shape[0], val_X.shape[1], 1)\n",
        "test_X_rnn = test_X.reshape(test_X.shape[0], test_X.shape[1], 1)\n",
        "\n",
        "rnn_model = build_rnn(train_X_rnn.shape[1])\n",
        "rnn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "rnn_model.fit(train_X_rnn, train_y, epochs=50, batch_size=32, validation_data=(val_X_rnn, val_y), verbose=1)\n",
        "\n",
        "\n",
        "# Evaluate RNN models\n",
        "rnn_predictions = rnn_model.predict(test_X_rnn)\n",
        "\n",
        "# Calculate MAE and RMSE for RNN\n",
        "rnn_mae = mean_absolute_error(test_y, rnn_predictions)\n",
        "rnn_rmse = mean_squared_error(test_y, rnn_predictions, squared=False)\n",
        "print(f\"RNN Mean Absolute Error: {rnn_mae}\")\n",
        "print(f\"RNN Root Mean Squared Error: {rnn_rmse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZM5QunILfDJ",
        "outputId": "15b739e3-23ca-471e-f719-d6f227e00689"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "86/86 [==============================] - 3s 14ms/step - loss: 0.1480 - val_loss: 0.0018\n",
            "Epoch 2/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 3.5034e-04\n",
            "Epoch 3/50\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 1.7562e-04 - val_loss: 1.1490e-04\n",
            "Epoch 4/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 1.2549e-04 - val_loss: 1.1621e-04\n",
            "Epoch 5/50\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 1.1504e-04 - val_loss: 1.2124e-04\n",
            "Epoch 6/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 1.0607e-04 - val_loss: 1.2196e-04\n",
            "Epoch 7/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 9.6319e-05 - val_loss: 9.1462e-05\n",
            "Epoch 8/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 8.7136e-05 - val_loss: 8.4140e-05\n",
            "Epoch 9/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 7.6549e-05 - val_loss: 9.0122e-05\n",
            "Epoch 10/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 6.9643e-05 - val_loss: 7.7499e-05\n",
            "Epoch 11/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 6.2900e-05 - val_loss: 6.3316e-05\n",
            "Epoch 12/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 5.3731e-05 - val_loss: 6.0890e-05\n",
            "Epoch 13/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 4.6869e-05 - val_loss: 6.1648e-05\n",
            "Epoch 14/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 4.2589e-05 - val_loss: 6.3609e-05\n",
            "Epoch 15/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 3.7696e-05 - val_loss: 5.1120e-05\n",
            "Epoch 16/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 3.5337e-05 - val_loss: 3.8774e-05\n",
            "Epoch 17/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 3.3155e-05 - val_loss: 3.7239e-05\n",
            "Epoch 18/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.9299e-05 - val_loss: 3.4648e-05\n",
            "Epoch 19/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.8271e-05 - val_loss: 3.3677e-05\n",
            "Epoch 20/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.7855e-05 - val_loss: 3.2358e-05\n",
            "Epoch 21/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.9583e-05 - val_loss: 3.0081e-05\n",
            "Epoch 22/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.6212e-05 - val_loss: 2.9669e-05\n",
            "Epoch 23/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.5862e-05 - val_loss: 2.8456e-05\n",
            "Epoch 24/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.6295e-05 - val_loss: 2.7666e-05\n",
            "Epoch 25/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 2.4641e-05 - val_loss: 2.7592e-05\n",
            "Epoch 26/50\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 2.6305e-05 - val_loss: 2.6428e-05\n",
            "Epoch 27/50\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 2.5154e-05 - val_loss: 2.9008e-05\n",
            "Epoch 28/50\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 2.4951e-05 - val_loss: 3.3952e-05\n",
            "Epoch 29/50\n",
            "86/86 [==============================] - 1s 11ms/step - loss: 2.4705e-05 - val_loss: 2.5967e-05\n",
            "Epoch 30/50\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 2.5053e-05 - val_loss: 2.9023e-05\n",
            "Epoch 31/50\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 2.4655e-05 - val_loss: 2.9345e-05\n",
            "Epoch 32/50\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 2.3940e-05 - val_loss: 3.0551e-05\n",
            "Epoch 33/50\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 2.4520e-05 - val_loss: 3.5015e-05\n",
            "Epoch 34/50\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 2.5540e-05 - val_loss: 2.9742e-05\n",
            "Epoch 35/50\n",
            "86/86 [==============================] - 1s 11ms/step - loss: 2.5149e-05 - val_loss: 2.6810e-05\n",
            "Epoch 36/50\n",
            "86/86 [==============================] - 1s 11ms/step - loss: 2.6239e-05 - val_loss: 3.2918e-05\n",
            "Epoch 37/50\n",
            "86/86 [==============================] - 1s 11ms/step - loss: 2.6129e-05 - val_loss: 4.7131e-05\n",
            "Epoch 38/50\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 2.7363e-05 - val_loss: 2.6373e-05\n",
            "Epoch 39/50\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 2.4323e-05 - val_loss: 2.4593e-05\n",
            "Epoch 40/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 2.8079e-05 - val_loss: 2.9761e-05\n",
            "Epoch 41/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.6727e-05 - val_loss: 2.9209e-05\n",
            "Epoch 42/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 2.2793e-05 - val_loss: 2.4468e-05\n",
            "Epoch 43/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 2.7330e-05 - val_loss: 3.1014e-05\n",
            "Epoch 44/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.9486e-05 - val_loss: 2.3968e-05\n",
            "Epoch 45/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.6193e-05 - val_loss: 2.5124e-05\n",
            "Epoch 46/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.7247e-05 - val_loss: 7.4331e-05\n",
            "Epoch 47/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 3.3449e-05 - val_loss: 2.8299e-05\n",
            "Epoch 48/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 2.5888e-05 - val_loss: 3.4517e-05\n",
            "Epoch 49/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 3.0011e-05 - val_loss: 4.2668e-05\n",
            "Epoch 50/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.7288e-05 - val_loss: 2.8940e-05\n",
            "47/47 [==============================] - 1s 4ms/step\n",
            "RNN Mean Absolute Error: 0.0040244600250981056\n",
            "RNN Root Mean Squared Error: 0.005922594251324852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM"
      ],
      "metadata": {
        "id": "Y5HneAVvagpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# LSTM Model\n",
        "def build_lstm(input_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        LSTM(64, input_shape=input_shape, activation='relu', return_sequences=True),\n",
        "        LSTM(32, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "train_X_lstm = train_X.reshape(train_X.shape[0], train_X.shape[1], 1)\n",
        "test_X_lstm = test_X.reshape(test_X.shape[0], test_X.shape[1], 1)\n",
        "\n",
        "lstm_model = build_lstm(train_X_lstm.shape[1:])\n",
        "lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "lstm_model.fit(train_X_lstm, train_y, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "lstm_predictions = lstm_model.predict(test_X_lstm)\n",
        "\n",
        "# Calculate MAE and RMSE for LSTM\n",
        "lstm_mae = mean_absolute_error(test_y, lstm_predictions)\n",
        "lstm_rmse = mean_squared_error(test_y, lstm_predictions, squared=False)\n",
        "print(f\"LSTM Mean Absolute Error: {lstm_mae}\")\n",
        "print(f\"LSTM Root Mean Squared Error: {lstm_rmse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQL3cHiCakCs",
        "outputId": "c6277c6e-8dc9-450f-dc80-bdef22e49095"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "86/86 [==============================] - 5s 7ms/step - loss: 0.1358\n",
            "Epoch 2/50\n",
            "86/86 [==============================] - 1s 11ms/step - loss: 0.0010\n",
            "Epoch 3/50\n",
            "86/86 [==============================] - 1s 12ms/step - loss: 1.7437e-04\n",
            "Epoch 4/50\n",
            "86/86 [==============================] - 1s 12ms/step - loss: 1.1775e-04\n",
            "Epoch 5/50\n",
            "86/86 [==============================] - 1s 12ms/step - loss: 8.3587e-05\n",
            "Epoch 6/50\n",
            "86/86 [==============================] - 1s 12ms/step - loss: 6.6323e-05\n",
            "Epoch 7/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 5.0098e-05\n",
            "Epoch 8/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 4.1293e-05\n",
            "Epoch 9/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 3.7431e-05\n",
            "Epoch 10/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 3.4915e-05\n",
            "Epoch 11/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 3.2976e-05\n",
            "Epoch 12/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 3.2701e-05\n",
            "Epoch 13/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 3.3361e-05\n",
            "Epoch 14/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 3.0873e-05\n",
            "Epoch 15/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 3.8799e-05\n",
            "Epoch 16/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 3.0319e-05\n",
            "Epoch 17/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 2.9014e-05\n",
            "Epoch 18/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 2.9317e-05\n",
            "Epoch 19/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 3.2067e-05\n",
            "Epoch 20/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 3.0837e-05\n",
            "Epoch 21/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 3.0308e-05\n",
            "Epoch 22/50\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 2.9715e-05\n",
            "Epoch 23/50\n",
            "86/86 [==============================] - 1s 11ms/step - loss: 3.1673e-05\n",
            "Epoch 24/50\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 3.0974e-05\n",
            "Epoch 25/50\n",
            "86/86 [==============================] - 1s 12ms/step - loss: 3.0299e-05\n",
            "Epoch 26/50\n",
            "86/86 [==============================] - 1s 12ms/step - loss: 2.8030e-05\n",
            "Epoch 27/50\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 3.0126e-05\n",
            "Epoch 28/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 3.0011e-05\n",
            "Epoch 29/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 2.9618e-05\n",
            "Epoch 30/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 3.3841e-05\n",
            "Epoch 31/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 3.5272e-05\n",
            "Epoch 32/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 3.2478e-05\n",
            "Epoch 33/50\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 3.6401e-05\n",
            "Epoch 34/50\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 2.8417e-05\n",
            "Epoch 35/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 2.9902e-05\n",
            "Epoch 36/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 2.8161e-05\n",
            "Epoch 37/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 3.0674e-05\n",
            "Epoch 38/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 2.8029e-05\n",
            "Epoch 39/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 3.1369e-05\n",
            "Epoch 40/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 3.2981e-05\n",
            "Epoch 41/50\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 3.2713e-05\n",
            "Epoch 42/50\n",
            "86/86 [==============================] - 1s 16ms/step - loss: 2.8897e-05\n",
            "Epoch 43/50\n",
            "86/86 [==============================] - 2s 23ms/step - loss: 3.0829e-05\n",
            "Epoch 44/50\n",
            "86/86 [==============================] - 1s 15ms/step - loss: 2.9724e-05\n",
            "Epoch 45/50\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 3.4094e-05\n",
            "Epoch 46/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 2.7754e-05\n",
            "Epoch 47/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 3.2189e-05\n",
            "Epoch 48/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 2.9289e-05\n",
            "Epoch 49/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 3.4134e-05\n",
            "Epoch 50/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 3.7327e-05\n",
            "47/47 [==============================] - 1s 4ms/step\n",
            "LSTM Mean Absolute Error: 0.004235892946474804\n",
            "LSTM Root Mean Squared Error: 0.006330051070529603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformers"
      ],
      "metadata": {
        "id": "sIOYM2fSbeIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# def build_transformer(input_shape, num_heads=4, ff_dim=32, num_transformer_blocks=4):\n",
        "#     inputs = Input(shape=input_shape)\n",
        "#     x = inputs\n",
        "#     for _ in range(num_transformer_blocks):\n",
        "#         attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=2)(x, x)\n",
        "#         attn_output = Reshape(target_shape=(-1, input_shape[1]))(attn_output)\n",
        "#         x = tf.keras.layers.Add()([x, attn_output])\n",
        "#         x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "#         ffn = tf.keras.Sequential([\n",
        "#             tf.keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
        "#             tf.keras.layers.Dense(input_shape[1])\n",
        "#         ])\n",
        "#         ffn_output = ffn(x)\n",
        "#         x = tf.keras.layers.Add()([x, ffn_output])\n",
        "#         x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "#     # outputs = Dense(1)(x)\n",
        "#     outputs = Dense(1, activation=\"linear\")(x)  # Linear activation for regression\n",
        "\n",
        "#     model = Model(inputs, outputs)\n",
        "#     return model\n",
        "\n",
        "\n",
        "# print(train_X.shape)\n",
        "# train_X_transformer = train_X.reshape(train_X.shape[0]*train_X.shape[1]//window_size, window_size, 1)\n",
        "# test_X_transformer = test_X.reshape(test_X.shape[0]*test_X.shape[1]//window_size, window_size, 1)\n",
        "\n",
        "# input_shape = (window_size, 1)\n",
        "# transformer_model = build_transformer(input_shape)\n",
        "# transformer_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# transformer_model.fit(train_X_transformer, train_y, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# transformer_predictions = transformer_model.predict(test_X_transformer)\n",
        "\n",
        "# # Calculate MAE and RMSE for Transformer\n",
        "# transformer_mae = mean_absolute_error(test_y, transformer_predictions)\n",
        "# transformer_rmse = mean_squared_error(test_y, transformer_predictions, squared=False)\n",
        "# print(f\"Transformer Mean Absolute Error: {transformer_mae}\")\n",
        "# print(f\"Transformer Root Mean Squared Error: {transformer_rmse}\")\n",
        "\n",
        "\n",
        "# input_shape = (train_X.shape[1],)\n",
        "# transformer_model = build_transformer(input_shape)\n",
        "# transformer_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# transformer_model.fit(train_X, train_y, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# transformer_predictions = transformer_model.predict(test_X)\n",
        "\n",
        "# # Calculate MAE and RMSE for Transformer\n",
        "# transformer_mae = mean_absolute_error(test_y, transformer_predictions)\n",
        "# transformer_rmse = mean_squared_error(test_y, transformer_predictions, squared=False)\n",
        "# print(f\"Transformer Mean Absolute Error: {transformer_mae}\")\n",
        "# print(f\"Transformer Root Mean Squared Error: {transformer_rmse}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Custom transformer layer\n",
        "# class TransformerLayer(tf.keras.layers.Layer):\n",
        "#     def __init__(self, num_heads, ff_dim, rate=0.1):\n",
        "#         super(TransformerLayer, self).__init__()\n",
        "#         self.multi_head_attention = MultiHeadAttention(num_heads=num_heads, key_dim=2)\n",
        "#         self.ffn = tf.keras.Sequential([\n",
        "#             Dense(ff_dim, activation=\"relu\"),\n",
        "#             Dense(input_shape[1])\n",
        "#         ])\n",
        "#         self.layer_norm1 = LayerNormalization(epsilon=1e-6)\n",
        "#         self.layer_norm2 = LayerNormalization(epsilon=1e-6)\n",
        "#         self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "#         self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "#     def call(self, inputs):\n",
        "#         attn_output = self.multi_head_attention(inputs, inputs)\n",
        "#         attn_output = self.dropout1(attn_output, training=True)\n",
        "#         out1 = self.layer_norm1(inputs + attn_output)\n",
        "#         ffn_output = self.ffn(out1)\n",
        "#         ffn_output = self.dropout2(ffn_output, training=True)\n",
        "#         return self.layer_norm2(out1 + ffn_output)\n",
        "\n",
        "\n",
        "# input_shape = (train_X.shape[1],)\n",
        "# num_transformer_blocks = 4\n",
        "# num_heads = 4\n",
        "# ff_dim = 32\n",
        "\n",
        "# inputs = Input(shape=input_shape)\n",
        "# x = inputs\n",
        "# for _ in range(num_transformer_blocks):\n",
        "#     x = TransformerLayer(num_heads=num_heads, ff_dim=ff_dim)(x)\n",
        "\n",
        "# outputs = Dense(1, activation=\"linear\")(x)  # Linear activation for regression\n",
        "# transformer_model = Model(inputs, outputs)\n",
        "\n",
        "# transformer_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# transformer_model.fit(train_X, train_y, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# transformer_predictions = transformer_model.predict(test_X)\n",
        "\n",
        "# # Calculate MAE and RMSE for Transformer\n",
        "# transformer_mae = mean_absolute_error(test_y, transformer_predictions)\n",
        "# transformer_rmse = mean_squared_error(test_y, transformer_predictions, squared=False)\n",
        "# print(f\"Transformer Mean Absolute Error: {transformer_mae}\")\n",
        "# print(f\"Transformer Root Mean Squared Error: {transformer_rmse}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense, MultiHeadAttention, LayerNormalization, Flatten\n",
        "\n",
        "# Custom transformer layer\n",
        "class TransformerLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerLayer, self).__init__()\n",
        "        self.multi_head_attention = MultiHeadAttention(num_heads=num_heads, key_dim=4)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation=\"relu\"),\n",
        "            Dense(4)\n",
        "        ])\n",
        "        self.layer_norm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layer_norm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attn_output = self.multi_head_attention(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=True)\n",
        "        out1 = self.layer_norm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=True)\n",
        "        return self.layer_norm2(out1 + ffn_output)\n",
        "\n",
        "\n",
        "input_shape = (4,)\n",
        "num_transformer_blocks = 4\n",
        "num_heads = 4\n",
        "ff_dim = 32\n",
        "\n",
        "def build_transformer(input_shape, num_transformer_blocks, num_heads, ff_dim):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=4)(x, x)\n",
        "        attn_output = LayerNormalization(epsilon=1e-6)(attn_output + x)\n",
        "\n",
        "        ffn = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation=\"relu\"),\n",
        "            Dense(4)\n",
        "        ])\n",
        "        ffn_output = ffn(attn_output)\n",
        "        x = LayerNormalization(epsilon=1e-6)(ffn_output + attn_output)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    outputs = Dense(1, activation=\"linear\")(x)  # Linear activation for regression\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "transformer_model = build_transformer(input_shape, num_transformer_blocks, num_heads, ff_dim)\n",
        "\n",
        "transformer_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "transformer_model.fit(train_X, train_y, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "transformer_predictions = transformer_model.predict(test_X)\n",
        "\n",
        "# Calculate MAE and RMSE for Transformer\n",
        "transformer_mae = mean_absolute_error(test_y, transformer_predictions)\n",
        "transformer_rmse = mean_squared_error(test_y, transformer_predictions, squared=False)\n",
        "print(f\"Transformer Mean Absolute Error: {transformer_mae}\")\n",
        "print(f\"Transformer Root Mean Squared Error: {transformer_rmse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "HOqsQ0zlbjdu",
        "outputId": "047fae39-35d0-4dd4-f814-398b7f2e7451"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-92f047d3761e>\u001b[0m in \u001b[0;36m<cell line: 173>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m \u001b[0mtransformer_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_transformer_blocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mff_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0mtransformer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-92f047d3761e>\u001b[0m in \u001b[0;36mbuild_transformer\u001b[0;34m(input_shape, num_transformer_blocks, num_heads, ff_dim)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_transformer_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLayerNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/layers/activation/softmax.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 )\n\u001b[1;32m    102\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Exception encountered when calling layer 'softmax' (type Softmax).\n\ntuple index out of range\n\nCall arguments received by layer 'softmax' (type Softmax):\n  • inputs=tf.Tensor(shape=(None, 4), dtype=float32)\n  • mask=None"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of train_X:\", train_X.shape)\n",
        "print(\"Shape of train_y:\", train_y.shape)\n",
        "print(\"Shape of test_X:\", test_X.shape)\n",
        "print(\"Shape of test_y:\", test_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9l3TS-0AczsF",
        "outputId": "c4831b63-829f-4b5b-ded6-86d581f12d80"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train_X: (2750, 4)\n",
            "Shape of train_y: (2750,)\n",
            "Shape of test_X: (1500, 4)\n",
            "Shape of test_y: (1500,)\n"
          ]
        }
      ]
    }
  ]
}