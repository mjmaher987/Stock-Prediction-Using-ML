{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2Vi9unCJVuSSOR7RYIq39",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjmaher987/Stock-Prediction-Using-ML/blob/main/Stock.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NBEATS"
      ],
      "metadata": {
        "id": "MZZagfKTJ-1V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "whfEf41xG0hA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load your DataFrame\n",
        "data = pd.read_csv('EURUSD_5min.csv')\n",
        "\n",
        "# Preprocessing\n",
        "scaler = MinMaxScaler()\n",
        "data['close'] = scaler.fit_transform(data['close'].values.reshape(-1, 1))\n",
        "\n",
        "# Splitting into training and testing sets\n",
        "train_size = int(0.8 * len(data))\n",
        "train_data, test_data = data[:train_size], data[train_size:]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n"
      ],
      "metadata": {
        "id": "1lrAm8-YdWhO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for N-BEATS\n",
        "window_size = 10\n",
        "X, y = [], []\n",
        "for i in range(len(data) - window_size):\n",
        "    X.append(data['close'][i:i+window_size])\n",
        "    y.append(data['close'][i+window_size])\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "train_size = int(0.8 * len(X))\n",
        "train_X, test_X = X[:train_size], X[train_size:]\n",
        "train_y, test_y = y[:train_size], y[train_size:]\n",
        "\n",
        "# Build N-BEATS model\n",
        "def build_nbeats(input_shape, output_shape, num_blocks=4, num_neurons=128):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    forecasts = []\n",
        "    for _ in range(num_blocks):\n",
        "        for _ in range(4):  # Directly following the N-BEATS architecture\n",
        "            x = Dense(num_neurons, activation='relu')(x)\n",
        "        forecast = Dense(output_shape[0])(x)\n",
        "        forecasts.append(forecast)\n",
        "    outputs = forecasts[0]  # Use the forecast from the first block as output\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "input_shape = (window_size, 1)\n",
        "output_shape = (1,)\n",
        "nbeats_model = build_nbeats(input_shape, output_shape)\n",
        "nbeats_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train N-BEATS model\n",
        "nbeats_model.fit(train_X, train_y, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# Make predictions using the trained model\n",
        "test_predictions = nbeats_model.predict(test_X)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14tLBzAMfiTa",
        "outputId": "05a506ba-4fb3-47d0-9981-e445b5874e13"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "125/125 [==============================] - 2s 6ms/step - loss: 0.0776\n",
            "Epoch 2/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0588\n",
            "Epoch 3/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0589\n",
            "Epoch 4/50\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0587\n",
            "Epoch 5/50\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0590\n",
            "Epoch 6/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0592\n",
            "Epoch 7/50\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0587\n",
            "Epoch 8/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0592\n",
            "Epoch 9/50\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.0585\n",
            "Epoch 10/50\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.0587\n",
            "Epoch 11/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.0591\n",
            "Epoch 12/50\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0591\n",
            "Epoch 13/50\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0586\n",
            "Epoch 14/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0583\n",
            "Epoch 15/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0585\n",
            "Epoch 16/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0586\n",
            "Epoch 17/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0583\n",
            "Epoch 18/50\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0583\n",
            "Epoch 19/50\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0582\n",
            "Epoch 20/50\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0582\n",
            "Epoch 21/50\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0586\n",
            "Epoch 22/50\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0585\n",
            "Epoch 23/50\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0582\n",
            "Epoch 24/50\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.0584\n",
            "Epoch 25/50\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.0583\n",
            "Epoch 26/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.0580\n",
            "Epoch 27/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0586\n",
            "Epoch 28/50\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.0584\n",
            "Epoch 29/50\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.0585\n",
            "Epoch 30/50\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0584\n",
            "Epoch 31/50\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.0583\n",
            "Epoch 32/50\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0582\n",
            "Epoch 33/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.0581\n",
            "Epoch 34/50\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0582\n",
            "Epoch 35/50\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0581\n",
            "Epoch 36/50\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.0586\n",
            "Epoch 37/50\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.0583\n",
            "Epoch 38/50\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0583\n",
            "Epoch 39/50\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.0582\n",
            "Epoch 40/50\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.0582\n",
            "Epoch 41/50\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.0582\n",
            "Epoch 42/50\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.0581\n",
            "Epoch 43/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.0581\n",
            "Epoch 44/50\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0585\n",
            "Epoch 45/50\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0583\n",
            "Epoch 46/50\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0582\n",
            "Epoch 47/50\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.0585\n",
            "Epoch 48/50\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.0581\n",
            "Epoch 49/50\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.0581\n",
            "Epoch 50/50\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.0583\n",
            "32/32 [==============================] - 1s 8ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reshaped_test_predictions = test_predictions[:, -1, :]"
      ],
      "metadata": {
        "id": "ILOv3ECcfC-x"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Calculate MAE and RMSE\n",
        "# print(test_y.shape)\n",
        "# print(reshaped_test_predictions.shape)\n",
        "mae = mean_absolute_error(test_y, reshaped_test_predictions)\n",
        "rmse = mean_squared_error(test_y, reshaped_test_predictions, squared=False)\n",
        "print(f\"Mean Absolute Error: {mae}\")\n",
        "print(f\"Root Mean Squared Error: {rmse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-OY7UI1d_mm",
        "outputId": "b968db5e-c9e9-4696-8c00-6d7f676bbabf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error: 0.25064638761832164\n",
            "Root Mean Squared Error: 0.2653645966964243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NHEATS"
      ],
      "metadata": {
        "id": "zhHMx5QtKDDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Load and preprocess your data\n",
        "# data = pd.read_csv('trading_data.csv')\n",
        "# scaler = MinMaxScaler()\n",
        "# data['close'] = scaler.fit_transform(data['close'].values.reshape(-1, 1))\n",
        "\n",
        "# Prepare features and target variable\n",
        "X = data[['open', 'high', 'low', 'volume']].values\n",
        "y = data['close'].values\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "train_size = int(0.7 * len(X))\n",
        "val_size = int(0.15 * len(X))\n",
        "train_X, test_X, train_y, test_y = X[:train_size], X[train_size:], y[:train_size], y[train_size:]\n",
        "train_X, val_X, train_y, val_y = train_X[:train_size - val_size], train_X[train_size - val_size:], \\\n",
        "                                  train_y[:train_size - val_size], train_y[train_size - val_size:]\n",
        "\n",
        "# Normalize data\n",
        "scaler_X = MinMaxScaler()\n",
        "train_X = scaler_X.fit_transform(train_X)\n",
        "val_X = scaler_X.transform(val_X)\n",
        "test_X = scaler_X.transform(test_X)\n",
        "\n",
        "scaler_y = MinMaxScaler()\n",
        "train_y = scaler_y.fit_transform(train_y.reshape(-1, 1)).flatten()\n",
        "val_y = scaler_y.transform(val_y.reshape(-1, 1)).flatten()\n",
        "test_y = scaler_y.transform(test_y.reshape(-1, 1)).flatten()\n",
        "\n",
        "# NHITS Model\n",
        "def build_nhits(input_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "nhits_model = build_nhits(train_X.shape[1])\n",
        "nhits_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "nhits_model.fit(train_X, train_y, epochs=50, batch_size=32, validation_data=(val_X, val_y), verbose=1)\n",
        "\n",
        "nhits_predictions = nhits_model.predict(test_X)\n",
        "\n",
        "# Calculate MAE and RMSE for NHITS\n",
        "nhits_mae = mean_absolute_error(test_y, nhits_predictions)\n",
        "nhits_rmse = mean_squared_error(test_y, nhits_predictions, squared=False)\n",
        "print(f\"NHITS Mean Absolute Error: {nhits_mae}\")\n",
        "print(f\"NHITS Root Mean Squared Error: {nhits_rmse}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARgE_1C1KEX5",
        "outputId": "ba8a1fd0-38c6-4e8e-9f0f-b02868ffe64f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "86/86 [==============================] - 3s 10ms/step - loss: 0.0499 - val_loss: 7.7747e-05\n",
            "Epoch 2/50\n",
            "86/86 [==============================] - 0s 4ms/step - loss: 4.1483e-05 - val_loss: 3.0732e-05\n",
            "Epoch 3/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.2980e-05 - val_loss: 2.1055e-05\n",
            "Epoch 4/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 2.2301e-05 - val_loss: 2.3493e-05\n",
            "Epoch 5/50\n",
            "86/86 [==============================] - 0s 4ms/step - loss: 2.0804e-05 - val_loss: 2.3018e-05\n",
            "Epoch 6/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.0567e-05 - val_loss: 1.9203e-05\n",
            "Epoch 7/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 1.8862e-05 - val_loss: 1.9334e-05\n",
            "Epoch 8/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 1.9572e-05 - val_loss: 2.0923e-05\n",
            "Epoch 9/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 1.8465e-05 - val_loss: 1.9600e-05\n",
            "Epoch 10/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.0846e-05 - val_loss: 2.7006e-05\n",
            "Epoch 11/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 2.4274e-05 - val_loss: 1.7884e-05\n",
            "Epoch 12/50\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 1.9990e-05 - val_loss: 2.0182e-05\n",
            "Epoch 13/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.1046e-05 - val_loss: 1.7042e-05\n",
            "Epoch 14/50\n",
            "86/86 [==============================] - 0s 4ms/step - loss: 1.9337e-05 - val_loss: 1.7105e-05\n",
            "Epoch 15/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 1.6640e-05 - val_loss: 1.7529e-05\n",
            "Epoch 16/50\n",
            "86/86 [==============================] - 0s 4ms/step - loss: 1.8241e-05 - val_loss: 2.2387e-05\n",
            "Epoch 17/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.6171e-05 - val_loss: 1.5879e-05\n",
            "Epoch 18/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.0035e-05 - val_loss: 2.1789e-05\n",
            "Epoch 19/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 1.7447e-05 - val_loss: 1.8748e-05\n",
            "Epoch 20/50\n",
            "86/86 [==============================] - 1s 11ms/step - loss: 1.8897e-05 - val_loss: 1.8919e-05\n",
            "Epoch 21/50\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 1.6718e-05 - val_loss: 1.5385e-05\n",
            "Epoch 22/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 1.6497e-05 - val_loss: 1.5246e-05\n",
            "Epoch 23/50\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 1.6597e-05 - val_loss: 1.7218e-05\n",
            "Epoch 24/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 1.5684e-05 - val_loss: 1.3971e-05\n",
            "Epoch 25/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.8288e-05 - val_loss: 1.5928e-05\n",
            "Epoch 26/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.8464e-05 - val_loss: 1.5581e-05\n",
            "Epoch 27/50\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 1.9186e-05 - val_loss: 1.5292e-05\n",
            "Epoch 28/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 2.2288e-05 - val_loss: 3.3111e-05\n",
            "Epoch 29/50\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 1.9830e-05 - val_loss: 1.3031e-05\n",
            "Epoch 30/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.7452e-05 - val_loss: 1.3592e-05\n",
            "Epoch 31/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.5411e-05 - val_loss: 1.4676e-05\n",
            "Epoch 32/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.7777e-05 - val_loss: 3.4328e-05\n",
            "Epoch 33/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.7782e-05 - val_loss: 1.4180e-05\n",
            "Epoch 34/50\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 2.2387e-05 - val_loss: 1.2361e-05\n",
            "Epoch 35/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.3926e-05 - val_loss: 4.1056e-05\n",
            "Epoch 36/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.7048e-05 - val_loss: 2.6230e-05\n",
            "Epoch 37/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.6265e-05 - val_loss: 1.5029e-05\n",
            "Epoch 38/50\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 1.6518e-05 - val_loss: 2.0223e-05\n",
            "Epoch 39/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 2.0633e-05 - val_loss: 1.2705e-05\n",
            "Epoch 40/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.5883e-05 - val_loss: 1.1056e-05\n",
            "Epoch 41/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.5029e-05 - val_loss: 1.8041e-05\n",
            "Epoch 42/50\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 1.5637e-05 - val_loss: 1.5757e-05\n",
            "Epoch 43/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 2.0340e-05 - val_loss: 2.4419e-05\n",
            "Epoch 44/50\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 1.5641e-05 - val_loss: 1.0729e-05\n",
            "Epoch 45/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.4354e-05 - val_loss: 2.0397e-05\n",
            "Epoch 46/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.5142e-05 - val_loss: 1.0273e-05\n",
            "Epoch 47/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.4733e-05 - val_loss: 1.2571e-05\n",
            "Epoch 48/50\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 1.4540e-05 - val_loss: 1.1100e-05\n",
            "Epoch 49/50\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 1.5237e-05 - val_loss: 1.6207e-05\n",
            "Epoch 50/50\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 1.7756e-05 - val_loss: 1.0195e-05\n",
            "47/47 [==============================] - 0s 1ms/step\n",
            "NHITS Mean Absolute Error: 0.002794559927064879\n",
            "NHITS Root Mean Squared Error: 0.0040522140072216575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RNN"
      ],
      "metadata": {
        "id": "QTwNc2NkLc1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN Model\n",
        "def build_rnn(input_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.LSTM(64, input_shape=(input_shape, 1), activation='relu'),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "train_X_rnn = train_X.reshape(train_X.shape[0], train_X.shape[1], 1)\n",
        "val_X_rnn = val_X.reshape(val_X.shape[0], val_X.shape[1], 1)\n",
        "test_X_rnn = test_X.reshape(test_X.shape[0], test_X.shape[1], 1)\n",
        "\n",
        "rnn_model = build_rnn(train_X_rnn.shape[1])\n",
        "rnn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "rnn_model.fit(train_X_rnn, train_y, epochs=50, batch_size=32, validation_data=(val_X_rnn, val_y), verbose=1)\n",
        "\n",
        "\n",
        "# Evaluate RNN models\n",
        "rnn_predictions = rnn_model.predict(test_X_rnn)\n",
        "\n",
        "# Calculate MAE and RMSE for RNN\n",
        "rnn_mae = mean_absolute_error(test_y, rnn_predictions)\n",
        "rnn_rmse = mean_squared_error(test_y, rnn_predictions, squared=False)\n",
        "print(f\"RNN Mean Absolute Error: {rnn_mae}\")\n",
        "print(f\"RNN Root Mean Squared Error: {rnn_rmse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZM5QunILfDJ",
        "outputId": "c9c25477-733b-4e93-eea4-c6939031440f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "86/86 [==============================] - 2s 10ms/step - loss: 0.1291 - val_loss: 0.0011\n",
            "Epoch 2/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 1.6221e-04\n",
            "Epoch 3/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 2.6662e-04 - val_loss: 1.4244e-04\n",
            "Epoch 4/50\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 2.0078e-04 - val_loss: 1.3773e-04\n",
            "Epoch 5/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 1.6405e-04 - val_loss: 1.3267e-04\n",
            "Epoch 6/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 1.4237e-04 - val_loss: 1.2163e-04\n",
            "Epoch 7/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 1.2792e-04 - val_loss: 1.0489e-04\n",
            "Epoch 8/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 1.1059e-04 - val_loss: 9.4011e-05\n",
            "Epoch 9/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 1.0030e-04 - val_loss: 8.7672e-05\n",
            "Epoch 10/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 9.4398e-05 - val_loss: 7.7016e-05\n",
            "Epoch 11/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 7.7477e-05 - val_loss: 7.3537e-05\n",
            "Epoch 12/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 7.0055e-05 - val_loss: 6.2216e-05\n",
            "Epoch 13/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 5.8282e-05 - val_loss: 5.7965e-05\n",
            "Epoch 14/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 4.9923e-05 - val_loss: 5.8287e-05\n",
            "Epoch 15/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 4.3145e-05 - val_loss: 4.4359e-05\n",
            "Epoch 16/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 3.7492e-05 - val_loss: 4.0230e-05\n",
            "Epoch 17/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 3.3772e-05 - val_loss: 3.5393e-05\n",
            "Epoch 18/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 3.2341e-05 - val_loss: 4.7058e-05\n",
            "Epoch 19/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.9902e-05 - val_loss: 4.0691e-05\n",
            "Epoch 20/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 2.8732e-05 - val_loss: 3.0361e-05\n",
            "Epoch 21/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 2.6893e-05 - val_loss: 3.0967e-05\n",
            "Epoch 22/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.6151e-05 - val_loss: 2.8660e-05\n",
            "Epoch 23/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.6994e-05 - val_loss: 2.9682e-05\n",
            "Epoch 24/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.6495e-05 - val_loss: 3.6350e-05\n",
            "Epoch 25/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.6459e-05 - val_loss: 2.7430e-05\n",
            "Epoch 26/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.5682e-05 - val_loss: 2.8729e-05\n",
            "Epoch 27/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 2.6319e-05 - val_loss: 2.7113e-05\n",
            "Epoch 28/50\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 2.5309e-05 - val_loss: 2.7211e-05\n",
            "Epoch 29/50\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 2.6374e-05 - val_loss: 3.5902e-05\n",
            "Epoch 30/50\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 2.8198e-05 - val_loss: 2.8495e-05\n",
            "Epoch 31/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 2.5051e-05 - val_loss: 2.7167e-05\n",
            "Epoch 32/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.5327e-05 - val_loss: 2.7673e-05\n",
            "Epoch 33/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 2.7351e-05 - val_loss: 3.0117e-05\n",
            "Epoch 34/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 2.8387e-05 - val_loss: 2.8259e-05\n",
            "Epoch 35/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.8438e-05 - val_loss: 2.6629e-05\n",
            "Epoch 36/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.7193e-05 - val_loss: 3.1654e-05\n",
            "Epoch 37/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.5784e-05 - val_loss: 2.7832e-05\n",
            "Epoch 38/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.8101e-05 - val_loss: 2.6469e-05\n",
            "Epoch 39/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.5279e-05 - val_loss: 2.9547e-05\n",
            "Epoch 40/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 2.7018e-05 - val_loss: 2.9041e-05\n",
            "Epoch 41/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 2.7762e-05 - val_loss: 2.6443e-05\n",
            "Epoch 42/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.6306e-05 - val_loss: 4.8401e-05\n",
            "Epoch 43/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.6514e-05 - val_loss: 3.8435e-05\n",
            "Epoch 44/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.5866e-05 - val_loss: 2.7621e-05\n",
            "Epoch 45/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.7983e-05 - val_loss: 2.7437e-05\n",
            "Epoch 46/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.6317e-05 - val_loss: 2.5975e-05\n",
            "Epoch 47/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.4789e-05 - val_loss: 2.5974e-05\n",
            "Epoch 48/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.6134e-05 - val_loss: 2.6710e-05\n",
            "Epoch 49/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 3.0022e-05 - val_loss: 2.5930e-05\n",
            "Epoch 50/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.5234e-05 - val_loss: 3.4252e-05\n",
            "47/47 [==============================] - 0s 2ms/step\n",
            "RNN Mean Absolute Error: 0.004333580381599212\n",
            "RNN Root Mean Squared Error: 0.006292262707347654\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM"
      ],
      "metadata": {
        "id": "Y5HneAVvagpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "\n",
        "# LSTM Model\n",
        "def build_lstm(input_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        LSTM(64, input_shape=input_shape, activation='relu', return_sequences=True),\n",
        "        LSTM(32, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "train_X_lstm = train_X.reshape(train_X.shape[0], train_X.shape[1], 1)\n",
        "test_X_lstm = test_X.reshape(test_X.shape[0], test_X.shape[1], 1)\n",
        "\n",
        "lstm_model = build_lstm(train_X_lstm.shape[1:])\n",
        "lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "lstm_model.fit(train_X_lstm, train_y, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "lstm_predictions = lstm_model.predict(test_X_lstm)\n",
        "\n",
        "# Calculate MAE and RMSE for LSTM\n",
        "lstm_mae = mean_absolute_error(test_y, lstm_predictions)\n",
        "lstm_rmse = mean_squared_error(test_y, lstm_predictions, squared=False)\n",
        "print(f\"LSTM Mean Absolute Error: {lstm_mae}\")\n",
        "print(f\"LSTM Root Mean Squared Error: {lstm_rmse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQL3cHiCakCs",
        "outputId": "97ac0bb8-2403-44cc-c103-49560c6eb042"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "86/86 [==============================] - 5s 8ms/step - loss: 0.1612\n",
            "Epoch 2/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0014\n",
            "Epoch 3/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 4.5147e-04\n",
            "Epoch 4/50\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 1.9850e-04\n",
            "Epoch 5/50\n",
            "86/86 [==============================] - 1s 12ms/step - loss: 1.2062e-04\n",
            "Epoch 6/50\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 1.0015e-04\n",
            "Epoch 7/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 9.5567e-05\n",
            "Epoch 8/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 9.3894e-05\n",
            "Epoch 9/50\n",
            "86/86 [==============================] - 1s 11ms/step - loss: 9.0039e-05\n",
            "Epoch 10/50\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 8.7696e-05\n",
            "Epoch 11/50\n",
            "86/86 [==============================] - 1s 11ms/step - loss: 8.6547e-05\n",
            "Epoch 12/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 8.4255e-05\n",
            "Epoch 13/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 8.3520e-05\n",
            "Epoch 14/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 7.6098e-05\n",
            "Epoch 15/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 7.5952e-05\n",
            "Epoch 16/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 7.1904e-05\n",
            "Epoch 17/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 7.4803e-05\n",
            "Epoch 18/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 7.3711e-05\n",
            "Epoch 19/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 7.2383e-05\n",
            "Epoch 20/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 6.8961e-05\n",
            "Epoch 21/50\n",
            "86/86 [==============================] - 1s 11ms/step - loss: 6.4152e-05\n",
            "Epoch 22/50\n",
            "86/86 [==============================] - 1s 11ms/step - loss: 6.2071e-05\n",
            "Epoch 23/50\n",
            "86/86 [==============================] - 1s 11ms/step - loss: 5.7680e-05\n",
            "Epoch 24/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 5.7576e-05\n",
            "Epoch 25/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 5.8923e-05\n",
            "Epoch 26/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 6.0004e-05\n",
            "Epoch 27/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 6.5340e-05\n",
            "Epoch 28/50\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 5.6961e-05\n",
            "Epoch 29/50\n",
            "86/86 [==============================] - 1s 15ms/step - loss: 4.8888e-05\n",
            "Epoch 30/50\n",
            "86/86 [==============================] - 1s 15ms/step - loss: 4.2569e-05\n",
            "Epoch 31/50\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 4.0962e-05\n",
            "Epoch 32/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 3.3307e-05\n",
            "Epoch 33/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 3.1367e-05\n",
            "Epoch 34/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 3.2435e-05\n",
            "Epoch 35/50\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 3.1195e-05\n",
            "Epoch 36/50\n",
            "86/86 [==============================] - 1s 15ms/step - loss: 3.1184e-05\n",
            "Epoch 37/50\n",
            "86/86 [==============================] - 2s 22ms/step - loss: 3.2224e-05\n",
            "Epoch 38/50\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 3.5471e-05\n",
            "Epoch 39/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 3.3273e-05\n",
            "Epoch 40/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 3.6699e-05\n",
            "Epoch 41/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 3.1341e-05\n",
            "Epoch 42/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 3.9966e-05\n",
            "Epoch 43/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 3.2992e-05\n",
            "Epoch 44/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 3.1894e-05\n",
            "Epoch 45/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 3.6641e-05\n",
            "Epoch 46/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 3.5833e-05\n",
            "Epoch 47/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 3.8798e-05\n",
            "Epoch 48/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 2.9364e-05\n",
            "Epoch 49/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 3.4572e-05\n",
            "Epoch 50/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 3.4092e-05\n",
            "47/47 [==============================] - 1s 5ms/step\n",
            "LSTM Mean Absolute Error: 0.006886983218155874\n",
            "LSTM Root Mean Squared Error: 0.008278957471336258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformers"
      ],
      "metadata": {
        "id": "sIOYM2fSbeIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, MultiHeadAttention, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def build_transformer(input_shape, num_heads=4, ff_dim=32, num_transformer_blocks=4):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=2)(x, x)\n",
        "        attn_output = Reshape(target_shape=(-1, input_shape[1]))(attn_output)\n",
        "        x = tf.keras.layers.Add()([x, attn_output])\n",
        "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(input_shape[1])\n",
        "        ])\n",
        "        ffn_output = ffn(x)\n",
        "        x = tf.keras.layers.Add()([x, ffn_output])\n",
        "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    outputs = Dense(1)(x)\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "train_X_transformer = train_X.reshape(train_X.shape[0], window_size, 1)\n",
        "test_X_transformer = test_X.reshape(test_X.shape[0], window_size, 1)\n",
        "\n",
        "input_shape = (window_size, 1)\n",
        "transformer_model = build_transformer(input_shape)\n",
        "transformer_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "transformer_model.fit(train_X_transformer, train_y, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "transformer_predictions = transformer_model.predict(test_X_transformer)\n",
        "\n",
        "# Calculate MAE and RMSE for Transformer\n",
        "transformer_mae = mean_absolute_error(test_y, transformer_predictions)\n",
        "transformer_rmse = mean_squared_error(test_y, transformer_predictions, squared=False)\n",
        "print(f\"Transformer Mean Absolute Error: {transformer_mae}\")\n",
        "print(f\"Transformer Root Mean Squared Error: {transformer_rmse}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def build_transformer(input_shape, num_heads=4, ff_dim=32, num_transformer_blocks=4):\n",
        "#     inputs = Input(shape=input_shape)\n",
        "#     x = inputs\n",
        "#     for _ in range(num_transformer_blocks):\n",
        "#         attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=2)(x, x)\n",
        "#         attn_output = Reshape(target_shape=(-1, input_shape[1]))(attn_output)\n",
        "#         x = layers.Add()([x, attn_output])\n",
        "#         x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "#         ffn = tf.keras.Sequential([\n",
        "#             layers.Dense(ff_dim, activation=\"relu\"),\n",
        "#             layers.Dense(input_shape[1])\n",
        "#         ])\n",
        "#         ffn_output = ffn(x)\n",
        "#         x = layers.Add()([x, ffn_output])\n",
        "#         x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "#     outputs = Dense(1)(x)\n",
        "#     model = Model(inputs, outputs)\n",
        "#     return model\n",
        "\n",
        "# train_X_transformer = train_X\n",
        "# test_X_transformer = test_X\n",
        "\n",
        "# input_shape = (window_size, 1)\n",
        "# transformer_model = build_transformer(input_shape)\n",
        "# transformer_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# transformer_model.fit(train_X_transformer, train_y, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# transformer_predictions = transformer_model.predict(test_X_transformer)\n",
        "\n",
        "# # Calculate MAE and RMSE for Transformer\n",
        "# transformer_mae = mean_absolute_error(test_y, transformer_predictions)\n",
        "# transformer_rmse = mean_squared_error(test_y, transformer_predictions, squared=False)\n",
        "# print(f\"Transformer Mean Absolute Error: {transformer_mae}\")\n",
        "# print(f\"Transformer Root Mean Squared Error: {transformer_rmse}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Transformer Model\n",
        "# def build_transformer(input_shape, num_heads=4, ff_dim=32, num_transformer_blocks=4):\n",
        "#     inputs = Input(shape=input_shape)\n",
        "#     x = inputs\n",
        "#     for _ in range(num_transformer_blocks):\n",
        "#         attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=2)(x, x)\n",
        "#         x = layers.ResidualNormalization()(attn_output)\n",
        "#         x = layers.PositionalEmbedding()(x)\n",
        "#         x = layers.FeedForward(ff_dim=ff_dim)(x)\n",
        "#     outputs = Dense(1)(x)\n",
        "#     model = Model(inputs, outputs)\n",
        "#     return model\n",
        "\n",
        "# train_X_transformer = train_X  # No reshaping needed for Transformer\n",
        "# test_X_transformer = test_X\n",
        "\n",
        "# transformer_model = build_transformer(train_X_transformer.shape[1:])\n",
        "# transformer_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# transformer_model.fit(train_X_transformer, train_y, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# transformer_predictions = transformer_model.predict(test_X_transformer)\n",
        "\n",
        "# # Calculate MAE and RMSE for Transformer\n",
        "# transformer_mae = mean_absolute_error(test_y, transformer_predictions)\n",
        "# transformer_rmse = mean_squared_error(test_y, transformer_predictions, squared=False)\n",
        "# print(f\"Transformer Mean Absolute Error: {transformer_mae}\")\n",
        "# print(f\"Transformer Root Mean Squared Error: {transformer_rmse}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Transformer Model\n",
        "# def build_transformer(input_shape, num_heads=4, ff_dim=32, num_transformer_blocks=4):\n",
        "#     inputs = Input(shape=input_shape)\n",
        "#     x = inputs\n",
        "#     for _ in range(num_transformer_blocks):\n",
        "#         x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=2)(x, x)\n",
        "#         x = layers.ResidualNormalization()(x)\n",
        "#         x = layers.PositionalEmbedding()(x)\n",
        "#         x = layers.FeedForward(ff_dim=ff_dim)(x)\n",
        "#     outputs = Dense(1)(x)\n",
        "#     model = Model(inputs, outputs)\n",
        "#     return model\n",
        "\n",
        "# transformer_model = build_transformer(train_X.shape[1:])\n",
        "# transformer_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# transformer_model.fit(train_X, train_y, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# transformer_predictions = transformer_model.predict(test_X)\n",
        "# # Calculate MAE and RMSE for Transformer\n",
        "# transformer_mae = mean_absolute_error(test_y, transformer_predictions)\n",
        "# transformer_rmse = mean_squared_error(test_y, transformer_predictions, squared=False)\n",
        "# print(f\"Transformer Mean Absolute Error: {transformer_mae}\")\n",
        "# print(f\"Transformer Root Mean Squared Error: {transformer_rmse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "HOqsQ0zlbjdu",
        "outputId": "c94c5c82-fc14-4624-d5e8-4d8886fe745a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-66f70541d8c3>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtrain_X_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mtest_X_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 11000 into shape (2750,10,1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of train_X:\", train_X.shape)\n",
        "print(\"Shape of test_X:\", test_X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9l3TS-0AczsF",
        "outputId": "30eb7d43-1bda-422c-8dfc-fce7952fd013"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train_X: (2750, 4)\n",
            "Shape of test_X: (1500, 4)\n"
          ]
        }
      ]
    }
  ]
}